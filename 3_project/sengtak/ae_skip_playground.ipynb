{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58510807-8708-4dbd-8a72-eabffb80dc62",
   "metadata": {},
   "source": [
    "# Notebook to experiment with different Conv Arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5449a7bc-6a3f-4116-b71c-83ec7a0dae87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import cv2\n",
    "\n",
    "import os, shutil\n",
    "from pathlib import Path\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968e5878-39e3-4ded-90e9-cad62609879b",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "* https://github.com/AntixK/PyTorch-VAE\n",
    "* https://ml-cheatsheet.readthedocs.io/en/latest/architectures.html#vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79095890",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipConnection_encoder(nn.Module):\n",
    "    def __init__(self, cache, index):\n",
    "        super(SkipConnection_encoder, self).__init__()\n",
    "        self.cache = cache\n",
    "        self.index = index\n",
    "    def forward(self, x):\n",
    "        self.cache[self.index] = x\n",
    "        return x\n",
    "\n",
    "class SkipConnection_decoder(nn.Module):\n",
    "    def __init__(self, cache, index):\n",
    "        super(SkipConnection_decoder, self).__init__()\n",
    "        self.cache = cache\n",
    "        self.index = index\n",
    "    def forward(self, x):\n",
    "        return x + self.cache[self.index]\n",
    "    \n",
    "#https://towardsdatascience.com/using-skip-connections-to-enhance-denoising-autoencoder-algorithms-849e049c0ac9    \n",
    "class AE_1_skipped(nn.Module):\n",
    "    def __init__(self, test_input, device):\n",
    "        super(AE_1_skipped, self).__init__()\n",
    "\n",
    "        if len(test_input.shape) == 3:\n",
    "            test_input = test_input.unsqueeze(dim=0)\n",
    "\n",
    "        channels = test_input.shape[1]\n",
    "        print(channels)\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.cache = [0,0,0]\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Dropout(p=0.2), #randomly drops 20% of input\n",
    "            nn.Conv2d(in_channels=channels, out_channels=64, kernel_size=5, stride=1, padding=2), #C1 out: 64, 400, 400\n",
    "            SkipConnection_encoder(self.cache, 0), #index 0, skipping to U4\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),#M1 out: 64, 200, 200\n",
    "            \n",
    "\n",
    "            # conv 2  \n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=5, stride=1, padding=2),#C2 out: 64, 200, 200\n",
    "            SkipConnection_encoder(self.cache, 1), #index 1, skipping to U3\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),#M2 out: 64, 100, 100\n",
    "\n",
    "            # conv 3\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),#C3 out: 128, 100, 100\n",
    "            SkipConnection_encoder(self.cache, 2), #index 2, skipping to U2\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),#M3 out: 128, 50, 50\n",
    "\n",
    "            # conv 4  \n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),#C4 out: 128, 50, 50\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),#M4 out: 128, 25, 25\n",
    "\n",
    "            # conv 5\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),#C5 out: 128, 25, 25\n",
    "            nn.LeakyReLU(),            \n",
    "        )\n",
    "              \n",
    "        self.decoder = nn.Sequential(\n",
    "            # conv 6\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear'), #U1 128, 50, 50\n",
    "            nn.ConvTranspose2d(in_channels=128, out_channels=128, kernel_size=5, stride=1, padding=2),#C6 out128, 50, 50\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            # conv 7\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear'), #U2 128, 100, 100\n",
    "            SkipConnection_decoder(self.cache, 2),\n",
    "            nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=5, stride=1, padding=2), #C7 64, 100, 100\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            # conv 8\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear'), #U3 64, 200, 200\n",
    "            SkipConnection_decoder(self.cache, 1),\n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=5, stride=1, padding=2), #C8 64, 200, 200\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            # conv 9\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear'), #U4 64, 400, 400\n",
    "            SkipConnection_decoder(self.cache, 0),\n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels=channels, kernel_size=3, stride=1, padding=1),  #C9 3, 400, 400\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        #test the size changes\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            x = self.encoder(test_input)\n",
    "            print('Encoded from ', test_input.shape, 'to', x.shape)\n",
    "            x = self.decoder(x)\n",
    "            print('output shape', x.shape)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.to(self.device)\n",
    "        return self.decoder( self.encoder(x) ) \n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z) \n",
    "    \n",
    "    def loss(self, x_input, x_ground_truth):\n",
    "        x_hat = self.forward(x_input)\n",
    "        #loss = F.binary_cross_entropy( x_hat, x_ground_truth )\n",
    "        loss = F.mse_loss( x_hat, x_ground_truth )\n",
    "        return loss, x_hat.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b61e135-3ce5-44ba-8bd9-ad751b007688",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from models.custom import AE_1_skipped, AE_2_skipped,AE_3_skipped, DAE1\n",
    "#%load_ext autoreload\n",
    "#%reload_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05222ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Encoded from  torch.Size([1, 1, 1024, 1024]) to torch.Size([1, 128, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sengtakgoh/miniforge3/lib/python3.8/site-packages/torch/nn/functional.py:3609: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape torch.Size([1, 1, 1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "test_input = torch.zeros((1,1,1024,1024))\n",
    "ae_model = AE_1_skipped(test_input=test_input, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a783f26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2014df648a12ccb51eb616f2bfb95dc97b0842176c9c1aef1b69282358c244cd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
